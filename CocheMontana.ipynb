{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faff2e9c",
   "metadata": {},
   "source": [
    "Nombre: Víctor\n",
    "\n",
    "Apellidos: Toscano Durán\n",
    "\n",
    "## Coche Montaña\n",
    "\n",
    "En el problema del coche de montaña un vehículo debe conducir hacia la derecha para salir de un valle. Las paredes del valle son lo suficientemente empinadas como para que acelerar a ciegas hacia la meta con una velocidad insuficiente haga que el vehículo se detenga y se deslice hacia abajo. El agente debe aprender a acelerar primero hacia la izquierda para ganar suficiente impulso en el retorno y poder subir la colina.\n",
    "\n",
    "El estado es la posición horizontal del vehículo x∈[−1.2,0.6] y la velocidad v∈[−0.07,0.07]. En cualquier paso de tiempo, el vehículo puede acelerar a la izquierda (a=−1), acelerar a la derecha (a=1), o dejarse llevar por la inercia (a=0). Recibimos una recompensa de −1 en cada cambio de sentido, y terminamos cuando el vehículo llega al lado derecho del valle más allá de x=0.6\n",
    ".\n",
    "Las transiciones en el problema del coche de montaña son deterministas:\n",
    "\n",
    "- v′ = v+0.001 a−0.0025cos(3x)                                                                                                 -- x' = x+v′\n",
    "\n",
    "El término gravitacional en la actualización de la velocidad es lo que impulsa al vehículo con poca potencia hacia el fondo del valle. Las transiciones se sujetan a los límites del espacio de estados.\n",
    "\n",
    "El problema del coche de montaña es un buen ejemplo de un problema con retorno retardado. Se requieren muchas acciones para llegar al estado objetivo, lo que dificulta que un agente no entrenado reciba algo más que penalizaciones unitarias de forma continua. Los mejores algoritmos de aprendizaje son capaces de propagar eficazmente el conocimiento de las trayectorias que llegan a la meta al resto del espacio de estados.                                                                                 Implementar esto en Python y obtener la politica optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33bd9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorToscano\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\VictorToscano\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\VictorToscano\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La política óptima alcanza la meta en promedio 1.0 veces en 10 episodios.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definir los límites del espacio de estados\n",
    "POSICION_MINIMA = -1.2\n",
    "POSICION_MAXIMA = 0.6\n",
    "VELOCIDAD_MINIMA = -0.07\n",
    "VELOCIDAD_MAXIMA = 0.07\n",
    "\n",
    "# Definir los hiperparámetros del algoritmo de Q-learning\n",
    "TASA_APRENDIZAJE = 0.1\n",
    "FACTO_DESCUENTO = 0.99\n",
    "EPSILON = 0.1\n",
    "NUM_EPISODIOS = 10000\n",
    "\n",
    "# Definir el número de divisiones para discretizar el espacio de estados\n",
    "NUM_POSICIONES = 30\n",
    "NUM_VELOCIDADES = 30\n",
    "NUM_ACCIONES = 3\n",
    "\n",
    "# Inicializar la tabla Q con valores aleatorios\n",
    "Q = np.random.uniform(low=-1, high=1, size=(NUM_POSICIONES, NUM_VELOCIDADES, NUM_ACCIONES))\n",
    "\n",
    "# Definir la función de discretización del espacio de estados\n",
    "def discretizar_estado(posicion, velocidad):\n",
    "    gap_posicion = (POSICION_MAXIMA - POSICION_MINIMA) / NUM_POSICIONES\n",
    "    gap_velocidad = (VELOCIDAD_MAXIMA - VELOCIDAD_MINIMA) / NUM_VELOCIDADES\n",
    "    \n",
    "    indice_posicion = int((posicion - POSICION_MINIMA) / gap_posicion)\n",
    "    indice_velocidad = int((velocidad - VELOCIDAD_MINIMA) / gap_velocidad)\n",
    "    \n",
    "    return indice_posicion, indice_velocidad\n",
    "\n",
    "# Definir la función de selección de acción\n",
    "def seleccionar_accion(estado):\n",
    "    if np.random.uniform() < EPSILON:\n",
    "        return np.random.randint(NUM_ACCIONES)\n",
    "    else:\n",
    "        return np.argmax(Q[estado[0], estado[1]])\n",
    "\n",
    "# Bucle principal de entrenamiento\n",
    "for episodio in range(NUM_EPISODIOS):\n",
    "    # Inicializar el estado del episodio\n",
    "    posicion = np.random.uniform(low=-0.6, high=-0.4)\n",
    "    velocidad = 0.0\n",
    "    estado = discretizar_estado(posicion, velocidad)\n",
    "    \n",
    "    # Bucle interno del episodio\n",
    "    while True:\n",
    "        # Seleccionar una acción\n",
    "        accion = seleccionar_accion(estado)\n",
    "        \n",
    "        # Tomar la acción y obtener la siguiente posición y velocidad\n",
    "        aceleracion = [-1.0, 0.0, 1.0][accion]\n",
    "        velocidad += 0.001 * aceleracion - 0.0025 * np.cos(3 * posicion)\n",
    "        velocidad = np.clip(velocidad, VELOCIDAD_MINIMA, VELOCIDAD_MAXIMA)\n",
    "        posicion += velocidad\n",
    "        posicion = np.clip(posicion, POSICION_MINIMA, POSICION_MAXIMA)\n",
    "        \n",
    "        # Obtener el nuevo estado discretizado\n",
    "        siguiente_estado = discretizar_estado(posicion, velocidad)\n",
    "        \n",
    "        # Obtener la recompensa\n",
    "        recompensa = -1 if posicion < 0.5 else 0\n",
    "        \n",
    "        # Actualizar la tabla Q\n",
    "        Q[estado][accion] += TASA_APRENDIZAJE * (recompensa + FACTO_DESCUENTO * np.max(Q[siguiente_estado]) - Q[estado][accion])\n",
    "        \n",
    "        # Actualizar el estado actual\n",
    "        estado = siguiente_estado\n",
    "        \n",
    "        # Terminar el episodio si se alcanza el estado objetivo\n",
    "        if posicion >= 0.5:\n",
    "            break\n",
    "\n",
    "# Evaluar la política óptima\n",
    "num_eval_episodios = 10\n",
    "recompensas_totales = 0\n",
    "\n",
    "for _ in range(num_eval_episodios):\n",
    "    posicion = np.random.uniform(low=-0.6, high=-0.4)\n",
    "    velocidad = 0.0\n",
    "    \n",
    "    while True:\n",
    "        estado = discretizar_estado(posicion, velocidad)\n",
    "        accion = np.argmax(Q[estado])\n",
    "        \n",
    "        aceleracion = [-1.0, 0.0, 1.0][accion]\n",
    "        velocidad += 0.001 * aceleracion - 0.0025 * np.cos(3 * posicion)\n",
    "        velocidad = np.clip(velocidad, VELOCIDAD_MINIMA, VELOCIDAD_MAXIMA)\n",
    "        posicion += velocidad\n",
    "        posicion = np.clip(posicion, POSICION_MINIMA, POSICION_MAXIMA)\n",
    "        \n",
    "        if posicion >= 0.5:\n",
    "            recompensas_totales += 1\n",
    "            break\n",
    "\n",
    "recompensa_promedio = recompensas_totales / num_eval_episodios\n",
    "print(f\"La política óptima alcanza la meta en promedio {recompensa_promedio} veces en {num_eval_episodios} episodios.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60c9bf",
   "metadata": {},
   "source": [
    "### Explicación código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf6f81",
   "metadata": {},
   "source": [
    "El codigo implementa el algoritmo de Q-learning para resolver el problem del coche de montaña. El objetivo es que el vehiculo aprenda a salir del valle acelerando primero hacia la izquierda para obtener suficiente impulso y luego subir la colina acelerando hacia la derecha.\n",
    "\n",
    "El bucle principal de entrenamiento ejecuta un numero determindo de episodios. En cada episodio, se inicializa el estado del vehiculo de manera aleatoria en terminos de posicion y velocidad. A continuacion, se ejecuta un bucle interno que continua hasta que el vehiculo alcanza la posicion objetivo.\n",
    "\n",
    "Dentro del bucle interno, se selecciona una accion basada en el estado actual del vehiculo. Se utiliza epsilon para equilibrar la exploracion y la explotacion. Si un numero aleatorio es menor que epsilon, se elige una accion al azar; de lo contrario, se selecciona la accion con el mayor valor de Q para ese estado.\n",
    "\n",
    "Una vez que se selecciona la accion, se actualiza la posicion y velocidad del vehiculo de acuerdo con las ecuaciones de transicion del problema del coche de montaña. Luego, se calcula la recompensa para el estado actual y se actualiza la tabla Q utilizando el algoritmo de Q-learning. La tabla Q almacena los valores de Q para cada estado y accion, y se actualiza en funcion de las recompensas obtenidas y los valores de Q para los estados siguientes.\n",
    "\n",
    "Despues de finalizar el bucle de entrenamiento, se evalua la politica optima obtenida. Esto implica ejecutar un numero de episodios de evaluacion y calcular la cantidad promedio de veces que el vehiculo alcanza la posicion objetivo.\n",
    "\n",
    "Finalmente, se muestra en pantalla el promedio de las veces que el vehiculo alcanza la posicion objetivo durante la evaluacion(similar, al de entrenamiento, pero seleccionando la mejor accion segun los valores de Q), lo que indica la efectividad de la politica optima aprendida."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
